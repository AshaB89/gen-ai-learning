{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Transformers by Building One from Scratch\n",
        "\n",
        "This notebook is a learning-oriented walkthrough of the Transformer\n",
        "architecture using a minimal PyTorch implementation.\n",
        "\n",
        "The goal is to understand *why* each component exists and how information\n",
        "flows through the model ‚Äî not to train a real language model.\n"
      ],
      "metadata": {
        "id": "8LnPOFCxZL_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Transformers?\n",
        "\n",
        "Sequence models like RNNs and LSTMs process tokens sequentially, which:\n",
        "- limits parallelism\n",
        "- makes long-range dependencies hard to learn\n",
        "\n",
        "Transformers remove recurrence entirely and rely on attention mechanisms\n",
        "to model relationships between tokens.\n"
      ],
      "metadata": {
        "id": "bkj_qYayZRCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook breaks the architecture into several key parts. Think of them like steps in a factory line:\n",
        "\n",
        "Token Embeddings: Computers don't understand words like \"Apple.\" They need numbers. This step turns each word into a list of numbers (a vector).\n",
        "\n",
        "Positional Encoding: Transformers process all words at once (unlike humans who read left-to-right). This step adds a \"time stamp\" to each word so the model knows where it sits in the sentence.\n",
        "\n",
        "Multi-Head Attention: This is the \"brain.\" It allows the model to look at the word \"bank\" and decide if it means a \"river bank\" or a \"money bank\" based on the surrounding words.\n",
        "\n",
        "Feedforward Layer: After the model \"pays attention\" to the context, this layer processes that information independently for each word to refine its meaning."
      ],
      "metadata": {
        "id": "BP-GEtaGdZKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåÄ The Transformer Mind Map: A Layman‚Äôs Guide\n",
        "Imagine the Transformer is a high-speed translation agency filled with workers. Instead of reading a book page by page, they tear the pages out and look at every word at the exact same time to understand the \"big picture\" instantly.\n",
        "\n",
        "1. The Input Stage: \"The Passport Office\"\n",
        "Before words enter the brain, they need two things: Identity and Location.\n",
        "\n",
        "Embeddings: We turn words into lists of numbers because computers can't \"read\" text.\n",
        "\n",
        "Positional Encoding: Since the model looks at all words at once, it loses the sense of order. We add a \"timestamp\" to each word so the model knows which word came first and which came last.\n",
        "\n",
        "2. The Interaction Stage: \"The Cocktail Party\" (Attention)\n",
        "This is the \"magic\" of the Transformer. Imagine all words in a sentence are at a party.\n",
        "\n",
        "Queries, Keys, and Values:\n",
        "\n",
        "Query: A word asking a question (e.g., \"Which word here describes me?\").\n",
        "\n",
        "Key: A word's \"ID badge\" (e.g., \"I am an adjective\").\n",
        "\n",
        "Value: The actual information the word holds.\n",
        "\n",
        "Self-Attention: Each word compares its Query against everyone else's Key to see who is relevant. If there's a match, it grabs that word's Value to update its own meaning.\n",
        "\n",
        "3. The Processing Stage: \"The Individual Thinking\" (Feedforward)\n",
        "Feedforward Network: After \"talking\" to other words in the Attention step, each word goes into a private booth to think.\n",
        "\n",
        "The Goal: It processes the new context it just learned (e.g., \"I now know that 'bank' refers to money, not a river\") independently of the other words.\n",
        "\n",
        "4. The Safety Net: \"The Quality Control\"\n",
        "Residual Connections: To make sure the original meaning of the word doesn't get \"garbled\" through too many layers, we keep a copy of the input and add it back to the output (like a \"shortcut\").\n",
        "\n",
        "Layer Normalization: This keeps the math from getting too wild or \"exploding,\" keeping the numbers in a healthy range for the next layer.\n",
        "\n",
        "5. The Output Stage: \"The Crystal Ball\"\n",
        "The Linear Layer: After several layers of \"talking\" and \"thinking,\" the model produces a final score for every word in its dictionary.\n",
        "\n",
        "Generation: It picks the word with the highest score, prints it out, and then feeds that word back into the start to figure out what the next word should be."
      ],
      "metadata": {
        "id": "vZb3Qxhqe32D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jqWAcQPVZD1H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Embeddings\n",
        "\n",
        "Language models operate on token IDs (integers), not raw text.\n",
        "Embeddings map each token ID to a dense vector representation.\n",
        "\n",
        "At this stage:\n",
        "- tokens have *no order*\n",
        "- tokens do *not* interact with each other\n"
      ],
      "metadata": {
        "id": "pCJuLkkvZWn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "\n",
        "Self-attention alone has no notion of sequence order.\n",
        "Positional encoding injects information about token positions.\n",
        "\n",
        "This implementation uses fixed sinusoidal encodings.\n"
      ],
      "metadata": {
        "id": "Bv7x99SFZKnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ],
      "metadata": {
        "id": "1pkvLKD3ZkqD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionalEncoding(d_model=16, max_len=10)\n",
        "dummy = torch.zeros(1, 10, 16)\n",
        "out = pe(dummy)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa2cRSwLZu0T",
        "outputId": "a6186b33-932b-47da-d3a5-e6596d0f0b3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We confirm that positional encodings preserve shape:\n",
        "(batch, sequence_length, embedding_dim)\n"
      ],
      "metadata": {
        "id": "NhYzsRquZyBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Attention: Intuition\n",
        "\n",
        "Self-attention allows each token to decide which other tokens\n",
        "are relevant when building its representation.\n",
        "\n",
        "Conceptually:\n",
        "- Query: what I‚Äôm looking for\n",
        "- Key: what I offer\n",
        "- Value: what information I provide\n"
      ],
      "metadata": {
        "id": "DvsGGOvgZ1h2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.k_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.v_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Combine heads\n",
        "        output = torch.matmul(attention, V).transpose(1, 2).contiguous()\n",
        "        output = output.view(batch_size, -1, self.d_model)\n",
        "        return self.fc_out(output)"
      ],
      "metadata": {
        "id": "4y6k35yfZzU-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = 2\n",
        "seq_len = 5\n",
        "d_model = 16\n",
        "heads = 4\n",
        "\n",
        "x = torch.randn(batch, seq_len, d_model)\n",
        "attn = MultiHeadAttention(d_model, heads)\n",
        "out = attn(x, x, x)\n",
        "\n",
        "print(out.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7jvC1xyZ_It",
        "outputId": "7cba0978-a9f7-41b9-ac65-320196f575c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 5, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention preserves the input shape:\n",
        "(batch, sequence_length, d_model)\n",
        "\n",
        "Internally, attention operates on\n",
        "(batch, heads, sequence_length, sequence_length)\n",
        "which explains its O(n¬≤) complexity."
      ],
      "metadata": {
        "id": "wVNNjln1aFIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward Layer: Why Attention Alone Isn‚Äôt Enough\n",
        "\n",
        "Self-attention allows tokens to exchange information with other tokens\n",
        "in the sequence. However, attention by itself is a **linear mixing**\n",
        "operation across tokens.\n",
        "\n",
        "Without an additional non-linear transformation, stacking attention\n",
        "layers would not significantly increase the model‚Äôs expressive power.\n",
        "\n",
        "The feedforward network (FFN) addresses this by:\n",
        "- applying a non-linear transformation\n",
        "- operating independently on each token\n",
        "- increasing representational capacity after attention\n",
        "\n",
        "Conceptually:\n",
        "- Attention answers *‚ÄúWhich tokens matter?‚Äù*\n",
        "- Feedforward layers answer *‚ÄúHow should this information be processed?‚Äù*\n",
        "\n",
        "Each Transformer layer includes a position-wise feedforward network\n",
        "that is applied to every token in the same way.\n"
      ],
      "metadata": {
        "id": "tZYdWAbBbns-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(torch.relu(self.linear1(x))))"
      ],
      "metadata": {
        "id": "7ngMT4PRZ__m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Block\n",
        "\n",
        "A Transformer layer combines:\n",
        "1. Self-attention (token interactions)\n",
        "2. Feedforward network (token-wise processing)\n",
        "3. Residual connections\n",
        "4. Layer normalization\n"
      ],
      "metadata": {
        "id": "gtClLhEqaRDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Multi-Head Attention + Residual\n",
        "        attn_output = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feedforward + Residual\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ypw_si_4aSFG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_len, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.fc_out(x)"
      ],
      "metadata": {
        "id": "PfcHgATObT3W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "vocab_size = 10000\n",
        "max_len = 100\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "num_layers = 6\n",
        "\n",
        "model = Transformer(d_model, num_heads, d_ff, num_layers, vocab_size, max_len)\n",
        "input_seq = torch.randint(0, vocab_size, (32, max_len))  # Batch size of 32\n",
        "output = model(input_seq)\n",
        "\n",
        "print(output.shape)  # Expected: (32, max_len, vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6voJhwKbWJN",
        "outputId": "c35eecc8-f79f-4fa1-db11-4b3908b4ac65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 100, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs logits for each token position and vocabulary item.\n",
        "(batch, sequence_length, vocab_size)\n"
      ],
      "metadata": {
        "id": "DDv90Dcfb4e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoregressive Generation\n",
        "\n",
        "Language models generate text by repeatedly predicting\n",
        "the next token given previous tokens.\n"
      ],
      "metadata": {
        "id": "ypmiepDJb69e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_token, max_len, vocab_size, device='cpu'):\n",
        "    model.eval()\n",
        "    generated = torch.tensor([start_token], dtype=torch.long, device=device).unsqueeze(0)\n",
        "    for _ in range(max_len):\n",
        "        output = model(generated)\n",
        "        next_token_logits = output[:, -1, :]\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "        generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n",
        "        if next_token.item() == vocab_size - 1:  # End token\n",
        "            break\n",
        "    return generated.squeeze().tolist()"
      ],
      "metadata": {
        "id": "yhMMyEjwb6vF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scope and Limitations\n",
        "\n",
        "This implementation:\n",
        "- is not trained\n",
        "- does not use causal masking\n",
        "- is not a production LLM\n",
        "\n",
        "Modern LLMs use the same architecture with optimized attention,\n",
        "causal masks, large-scale training, and pretrained weights.\n"
      ],
      "metadata": {
        "id": "Dj2qXLTWcMrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 100  # Example vocabulary size (including <END>)\n",
        "d_model = 128\n",
        "num_heads = 4\n",
        "d_ff = 256\n",
        "num_layers = 3\n",
        "max_len = 50\n",
        "\n",
        "# Instantiate and move model to device\n",
        "device = 'cpu'\n",
        "model = Transformer(d_model, num_heads, d_ff, num_layers, vocab_size, max_len).to(device)\n",
        "\n",
        "# Example usage\n",
        "start_token = 1  # Assume 1 is the start token\n",
        "output = generate_text(model, start_token, max_len=20, vocab_size=vocab_size, device=device)\n",
        "print(\"Generated Text:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwRUnBf8cKKF",
        "outputId": "c5067707-d35e-49fe-90ca-e75d58f43ab4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: [1, 7, 25, 8, 16, 23, 54, 22, 18, 24, 17, 35, 46, 43, 6, 35, 45, 77, 46, 43, 92]\n"
          ]
        }
      ]
    }
  ]
}